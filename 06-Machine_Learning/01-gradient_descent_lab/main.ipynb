{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea5cf67d-8299-47d1-9266-571c25c0a83d",
   "metadata": {},
   "source": [
    "Goals\n",
    "In this lab, you will:\n",
    "\n",
    "automate the process of optimizing  𝑤\n",
    "  and  𝑏\n",
    "  using gradient descent.\n",
    "Tools\n",
    "In this lab, we will make use of:\n",
    "\n",
    "NumPy, a popular library for scientific computing\n",
    "Matplotlib, a popular library for plotting data\n",
    "plotting routines in the lab_utils.py file in the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7c26fe5-148a-4e6c-806f-b977c08118ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('./deeplearning.mplstyle')\n",
    "# from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67973008-99d4-40ee-89d0-9fe68b65215c",
   "metadata": {},
   "source": [
    "Problem Statement\n",
    "Let's use the same two data points as before - a house with 1000 square feet sold for $300,000 and a house with 2000 square feet sold for $500,000.\n",
    "\n",
    "Size (1000 sqft)\tPrice (1000s of dollars)\n",
    "1\t300\n",
    "2\t500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff4bf555-db52-4e10-8590-1e40ce3e574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load our data set\n",
    "\n",
    "x_train = np.array([1.0,2.0])\n",
    "y_train = np.array([300.0,500.0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153c074-6c2f-4faa-b139-708af6b82b56",
   "metadata": {},
   "source": [
    "Compute_Cost\n",
    "This was developed in the last lab. We'll need it again here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91211d7f-14e4-42fd-a327-e036d1c49399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Function to calculate the cost\n",
    "\n",
    "def compute_cost(x,y,w,b):\n",
    "    m = x.shape[0]\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost = 1/(2*m)*cost\n",
    "    return total_cost\n",
    "\n",
    "print(compute_cost(x_train,y_train,200,100))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50be80e-8cef-4bd1-9e64-3f3b4dcd8acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f6cfeb4-83f9-4b82-b565-5329abb5fb92",
   "metadata": {},
   "source": [
    "Gradient descent summary\n",
    "So far in this course, you have developed a linear model that predicts  𝑓𝑤,𝑏(𝑥(𝑖))\n",
    " :\n",
    "𝑓𝑤,𝑏(𝑥(𝑖))=𝑤𝑥(𝑖)+𝑏(1)\n",
    "In linear regression, you utilize input training data to fit the parameters  𝑤\n",
    " , 𝑏\n",
    "  by minimizing a measure of the error between our predictions  𝑓𝑤,𝑏(𝑥(𝑖))\n",
    "  and the actual data  𝑦(𝑖)\n",
    " . The measure is called the  𝑐𝑜𝑠𝑡\n",
    " ,  𝐽(𝑤,𝑏)\n",
    " . In training you measure the cost over all of our training samples  𝑥(𝑖),𝑦(𝑖)\n",
    " \n",
    "𝐽(𝑤,𝑏)=12𝑚∑𝑖=0𝑚−1(𝑓𝑤,𝑏(𝑥(𝑖))−𝑦(𝑖))2(2)\n",
    "\n",
    "In lecture, gradient descent was described as:\n",
    "\n",
    "repeat𝑤𝑏} until convergence:{=𝑤−𝛼∂𝐽(𝑤,𝑏)∂𝑤=𝑏−𝛼∂𝐽(𝑤,𝑏)∂𝑏(3)\n",
    "where, parameters 𝑤\n",
    ", 𝑏\n",
    " are updated simultaneously.\n",
    "The gradient is defined as:\n",
    "∂𝐽(𝑤,𝑏)∂𝑤∂𝐽(𝑤,𝑏)∂𝑏=1𝑚∑𝑖=0𝑚−1(𝑓𝑤,𝑏(𝑥(𝑖))−𝑦(𝑖))𝑥(𝑖)=1𝑚∑𝑖=0𝑚−1(𝑓𝑤,𝑏(𝑥(𝑖))−𝑦(𝑖))(4)(5)\n",
    "\n",
    "Here simultaniously means that you calculate the partial derivatives for all the parameters before updating any of the parameters.\n",
    "\n",
    "\n",
    "Implement Gradient Descent\n",
    "You will implement gradient descent algorithm for one feature. You will need three functions.\n",
    "\n",
    "compute_gradient implementing equation (4) and (5) above\n",
    "compute_cost implementing equation (2) above (code from previous lab)\n",
    "gradient_descent, utilizing compute_gradient and compute_cost\n",
    "Conventions:\n",
    "\n",
    "The naming of python variables containing partial derivatives follows this pattern,∂𝐽(𝑤,𝑏)∂𝑏\n",
    " will be dj_db.\n",
    "w.r.t is With Respect To, as in partial derivative of 𝐽(𝑤𝑏)\n",
    " With Respect To 𝑏\n",
    ".\n",
    "\n",
    "compute_gradient\n",
    "compute_gradient implements (4) and (5) above and returns ∂𝐽(𝑤,𝑏)∂𝑤\n",
    ",∂𝐽(𝑤,𝑏)∂𝑏\n",
    ". The embedded comments describe the operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ae5a1c-980e-477d-af22-eb6898f8efab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x,y,w,b):\n",
    "    # number of training examples\n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        dj_dw_i = (f_wb - y[i])*x[i]\n",
    "        dj_db_i = (f_wb - y[i])\n",
    "        dj_dw += dj_dw_i\n",
    "        dj_db += dj_db_i\n",
    "    dj_dw = dj_dw/m\n",
    "    dj_db = dj_db/m\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "# practice\n",
    "def compute_gradient(x,y,w,b):\n",
    "    \n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        dj_dw_i = (f_wb - y[i]) * x[i]\n",
    "        dj_db_i = (f_wb - y[i])\n",
    "        dj_dw += dj_dw_i\n",
    "        dj_db += dj_db_i\n",
    "    dj_dw = dj_dw/m\n",
    "    dj_db = dj_db/m\n",
    "    return dj_dw ,dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e7aae-fafa-4f50-928b-06d1173b01bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y,w_in,b_in,alpha,num_iters,cost_function,gradient_function):\n",
    "    #an array to store cost j and w at each iteration primarily for graphing later\n",
    "    j_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    for i in range(num_iters):\n",
    "        #Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw , dj_db = gradient_function(x,y,w,b)\n",
    "        #update parameters using eqn 3 above\n",
    "        b = b - alpha * dj_db\n",
    "        w = w - alpha * dj_dw\n",
    "        #save cost j at each iteration\n",
    "        if i < 100000:\n",
    "            j_history.append(cost_fuction(x,y,w,b))\n",
    "            p_history.append([w,b])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
